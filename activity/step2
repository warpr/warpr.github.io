#!/usr/bin/env python2

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

# requirements:
# sudo apt-get install python-html5lib python-lxml python-iso8601 python-tz

import codecs
import datetime
import iso8601
import itertools
import json
import lxml.html
import pytz
import re
import requests
from collections import namedtuple

Activity = namedtuple(
    "Activity", "avatar title summary url datetime type level service")


def parse_html (filename):
    with codecs.open (filename, "rb", "UTF-8") as f:
        return lxml.html.fromstring (f.read ().encode ("UTF-8"))


def parse_xml (filename):
    with codecs.open (filename, "rb", "UTF-8") as f:
        return lxml.etree.parse (f)


def parse_json (filename):
    with codecs.open (filename, "rb", "UTF-8") as f:
        return json.loads (f.read ())


def fix_links (base_uri, dom):
    for a in dom.cssselect ('a'):
        if 'href' not in a.attrib:
            continue

        if a.attrib['href'].startswith ('/'):
            a.attrib['href'] = base_uri + a.attrib['href']


def trim_whitespace (dom):
    ws = re.compile (r'\W+')

    for elem in dom.iter ('*'):
        if elem.text is not None:
            elem.text = ws.sub (' ', elem.text)
        if elem.tail is not None:
            elem.tail = ws.sub (' ', elem.tail)


def prune_class (css_class, skip):
    classes = css_class.split (' ')
    for cls in classes:
        if cls not in skip:
            return cls
    return None


def tweet_timestamp (url):
    print ("Getting timestamp for tweet", url)

    response = requests.get ('https://twitter.com' + url)
    dom = lxml.html.fromstring (response.content)

    span = dom.cssselect ('span[data-time]')[0]
    timestamp = int(span.attrib['data-time'])
    # timestamp = 1383215546
    dt = datetime.datetime.utcfromtimestamp(timestamp)
    return dt.replace (tzinfo=pytz.utc)


def twitter (filename):
    base_uri = 'https://twitter.com'

    dom = parse_html (filename)
    fix_links (base_uri, dom)

    for tweet in dom.cssselect ('table.tweet')[0:5]:
        tweet_content = tweet.cssselect ('.tweet-text')[0]
        tweet_context = tweet.cssselect ('.tweet-social-context span.context')
        retweet = True if tweet_context else False

        title = tweet_context[0].text if retweet else None

        yield Activity(
            avatar=tweet.cssselect ('.avatar img')[0].attrib['src'],
            title=title,
            url=base_uri + tweet.attrib['href'],
            summary=lxml.html.tostring (tweet_content),
            datetime=tweet_timestamp (tweet.attrib['href']),
            level="minor" if retweet else "major",
            type="tweet",
            service="twitter")


def bitbucket (filename):
    base_uri = 'https://bitbucket.org'

    dom = parse_html (filename)
    fix_links (base_uri, dom)

    for article in dom.cssselect ('article.news-item'):
        desc = article.cssselect ('p')[0]
        trim_whitespace (desc)

        changeset = article.cssselect('div.changeset a')
        url = changeset[0].attrib['href'] if changeset else None

        dt = iso8601.parse_date (article.cssselect ('time')[0].attrib['datetime'])

        yield Activity(
            avatar=article.cssselect ('img.avatar')[0].attrib['src'],
            title=None,
            url=url,
            summary=lxml.html.tostring (desc),
            datetime=dt,
            level="minor",
            type=prune_class (article.attrib['class'], [ 'news-item' ]),
            service="bitbucket")


def github_issue_comment_event (event):
    summary = ('<a href="%s">%s</a> commented on ' +
            '<a href="%s">%s</a> <a href="%s">issue #%s</a>.') % (
                event["actor"]["url"],
                event["actor"]["login"],
                event["repo"]["url"],
                event["repo"]["name"],
                event["payload"]["issue"]["url"],
                event["payload"]["issue"]["number"])

    return (event["payload"]["issue"]["url"], summary)


def github_push_event (event):
    topic = "a commit"
    if event["payload"]["size"] > 1:
        topic = unicode(event["payload"]["size"]) + " commits"

    summary = ('<a href="%s">%s</a> pushed ' +
            '<a href="%s">%s</a> to <a href="%s">%s</a>.') % (
                event["actor"]["url"],
                event["actor"]["login"],
                event["payload"]["commits"][0]["url"],
                topic,
                event["repo"]["url"],
                event["repo"]["name"])

    return (event["payload"]["commits"][0]["url"], summary)


def github_pull_request_event (event):
    summary = ('<a href="%s">%s</a> submitted ' +
               '<a href="%s">pull request #%s</a> to <a href="%s">%s</a>.') % (
                event["actor"]["url"],
                event["actor"]["login"],
                event["payload"]["pull_request"]["url"],
                event["payload"]["pull_request"]["number"],
                event["repo"]["url"],
                event["repo"]["name"])

    return (event["payload"]["pull_request"]["url"], summary)


def github_create_event (event):
    summary = ('<a href="%s">%s</a> created ' +
            '%s %s in <a href="%s">%s</a>.') % (
                event["actor"]["url"],
                event["actor"]["login"],
                event["payload"]["ref_type"],
                event["payload"]["ref"],
                event["repo"]["url"],
                event["repo"]["name"])

    return (event["repo"]["url"], summary)


def github (filename):
    data = parse_json (filename)

    activity = {}
    activity["PushEvent"] = github_push_event
    activity["IssueCommentEvent"] = github_issue_comment_event
    activity["PullRequestEvent"] = github_pull_request_event
    activity["CreateEvent"] = github_create_event

    level = {}
    level["PushEvent"] = "minor"
    level["IssueCommentEvent"] = "minor"
    level["PullRequestEvent"] = "minor"
    level["CreateEvent"] = "minor"

    for event in data:
        if event["type"] not in activity:
            continue

        (url, summary) = activity[event["type"]](event)

        yield Activity(
            avatar=event["actor"]["avatar_url"],
            title=None,
            summary=summary,
            url=url,
            type=event["type"],
            level=level[event["type"]],
            datetime=iso8601.parse_date (event["created_at"]),
            service="github")


def pump (filename):
    data = parse_json (filename)

    for event in data["minor"]["items"]:
        yield Activity(
            avatar=event["actor"]["image"]["url"],
            title=None,
            url=event.get ("url", None),
            summary=event["content"],
            type=event["verb"],
            level="minor",
            datetime=iso8601.parse_date (event["updated"]),
            service="pump.io")

    for event in data["major"]["items"]:
        avatar=event["actor"]["image"]["url"]
        title=None
        title=event["content"]

        if "content" in event.get ("object", {}):
            if "author" in event["object"]:
                avatar=event["object"]["author"]["image"]["url"]
            title=event["content"]
            summary=event["object"]["content"]


        yield Activity(
            avatar=avatar,
            title=title,
            url=event.get ("url", None),
            summary=summary,
            type=event["verb"],
            level="major",
            datetime=iso8601.parse_date (event["updated"]),
            service="pump.io")


def stackoverflow (filename):
    dom = parse_xml (filename)
    ns = { 'atom': 'http://www.w3.org/2005/Atom' }

    for entry in dom.xpath ('//atom:entry', namespaces=ns):
        url = entry.find('atom:link', ns).attrib['href']
        title = entry.find('atom:title', ns).text

        yield Activity(
            avatar=None,
            title=None,
            url=url,
            summary=title,
            datetime=iso8601.parse_date (entry.find('atom:published', ns).text),
            type="post", level="major", service="stackoverflow")


def devblog (filename):
    dom = parse_xml (filename)
    ns = { 'atom': 'http://www.w3.org/2005/Atom' }

    for entry in dom.xpath ('//atom:entry', namespaces=ns):
        url = entry.find('atom:link', ns).attrib['href']
        title = entry.find('atom:title', ns).text
        summary = entry.find('atom:summary', ns).text

        yield Activity(
            avatar=None, title=title, url=url, summary=summary,
            datetime=iso8601.parse_date (entry.find('atom:updated', ns).text),
            type="post", level="major", service="devblog")


def cmp_datetime (a, b):
    return int ((a.datetime - b.datetime).total_seconds ())


def format_timestamp (event):
    event["datetime"] = event["datetime"].isoformat ()
    return event


def write_results (filename, events):
    output = json.dumps ({
        "data": [
            format_timestamp (e._asdict ()) for e in events
        ]}, indent=4)

    print ("Writing to", filename)

    with codecs.open (filename, "wb", "UTF-8") as f:
        f.write (output)


if __name__ == '__main__':
    print ("Parsing events...")

    events = sorted (itertools.chain (
        bitbucket ("bitbucket.events.html"),
        github ("github.events.json"),
        pump ("identi.ca.json"),
        devblog ("blog.atom.xml"),
        stackoverflow ("stackoverflow.atom.xml"),
        twitter ("twitter.events.html")
    ), cmp=cmp_datetime, reverse=True)

    write_results ("events.json", events)

    major = filter (lambda x: x.level == "major", events)
    minor = filter (lambda x: x.level == "minor", events)

    write_results ("minor.json", minor)
    write_results ("major.json", major)
